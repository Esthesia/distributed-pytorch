{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributed Mixed-precision Training with PyTorch and NVIDIA `Apex`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is `Apex`?\n",
    "A Pytorch extension with NVIDIA-maintained utilities to streamline mixed precision and distributed training. It has the full features of the built-in PyTorch Distributed Data Parallel (DDP) package. Additionally, it integrates better with NVIDIA GPUs and provides mixed-precision training acceleration.\n",
    "\n",
    "Most deep learning frameworks, including PyTorch, train using 32-bit floating point (FP32) arithmetic by default. However, using FP32 for all operations is not essential to achieve full accuracy for many state-of-the-art deep neural networks (DNNs). In mixed precision training, majority of the network uses FP16 arithmetic, while automatically casting potentially unstable operations to FP32.\n",
    "\n",
    "Key points:\n",
    "- Ensuring that weight updates are carried out in FP32.\n",
    "- Loss scaling to prevent underflowing gradients.\n",
    "- A few operations (e.g. large reductions) left in FP32.\n",
    "- Everything else (the majority of the network) executed in FP16."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why `Apex`?\n",
    "\n",
    "- comes with all the distributed training features of the built-in PyTorch DDP\n",
    "- better performance than built-in DDP\n",
    "- reducing memory storage/bandwidth demands by 2x\n",
    "- use larger batch sizes\n",
    "- take advantage of NVIDIA Tensor Cores for matrix multiplications and convolutions\n",
    "- don't need to explicitly convert your model, or the input data, to half()."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to use `Apex`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bells and Whistles\n",
    "\n",
    "### How to prevent race condition when mutiple devices try to do logging or printing?\n",
    "\n",
    "### How to use `Tensorboard` in a distributed context?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The full ImageNet training script\n",
    "Please see: `imagenet_ddp_apex.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
